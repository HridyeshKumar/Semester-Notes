How to handle Continuous Valued Attributes in Decision Tree

Youtube Link:

https://youtu.be/2vIvM4zmyf4?feature=shared

ChatGpt Link:

https://chat.openai.com/share/dc6176fa-aa14-4912-8136-5f3f856b5577


Q 3 For a3 which is a continuous attributes computes the information gain.

Instance         a1             a2              a3           Target Class

1                T               T               1                      +
2                T               T               6                      +
3                T               F               5                      -
4                F               F               4                      +         
5                F               T               7                      -
6                F               T               3                      -
7                F               F               8                      -
8                T               F               7                      +
9                F               T               5                      -



Q 3 Solution

How to handle Continuous Valued Attributes in Decision Tree

To compute the information gain for a continuous attribute, we need to first convert it into a discrete attribute by creating intervals or bins. 

In this case, we can create two bins for attribute a3: one for values less than or equal to 5, and another for values greater than 5. 

Next, we calculate the entropy for each bin by counting the number of positive and negative instances in each bin. 

For the first bin (<= 5):
- Positive instances: 1 (instance 3)
- Negative instances: 2 (instances 5 and 6)

Entropy for the first bin = - (1/3) * log2(1/3) - (2/3) * log2(2/3) ≈ 0.918

For the second bin (> 5):
- Positive instances: 2 (instances 2 and 8)
- Negative instances: 2 (instances 3 and 9)

Entropy for the second bin = - (2/4) * log2(2/4) - (2/4) * log2(2/4) = 1

Next, we calculate the weighted average entropy by considering the number of instances in each bin. 

Weighted average entropy = (3/9) * 0.918 + (6/9) * 1 ≈ 0.972

Finally, we calculate the information gain by subtracting the weighted average entropy from the entropy of the target class.

Information gain = 1 - 0.972 ≈ 0.028

Therefore, the information gain for attribute a3 is approximately 0.028.